{
  "shard_id": "language_model_v2_tokenizer",
  "version": "2.0.0",
  "size_bytes": 3557680,
  "checksum": "1fe93b6152957cf9cfd6d89002467f789ce8b3f3e000b3a2edf27c808ddd0b9e",
  "model_type": "tokenizer",
  "created_at": "2025-08-10T01:35:05Z",
  "updated_at": "2025-08-10T01:35:05Z",
  "description": "Tokenizer for language_model_v2 v2",
  "tags": [
    "tokenizer",
    "huggingface",
    "language_model_v2"
  ],
  "dependencies": []
}